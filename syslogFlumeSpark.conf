#syslogFlume.conf file to log from multiple machines to hdfs 
#using flume source type syslogtcp

a1.sources = r1
a1.sinks = k1 k2 #k1 hdfsSink, k2 Spark streaming sink
a1.channels = c1 c2 #c1 hdfsChannel, c2 spark streaming sink

#source r1 configuration
a1.sources.r1.type = multiport_syslogtcp
a1.sources.r1.ports = 515 516
a1.sources.r1.host = 192.168.54.181
a1.sources.r1.charset.default = UTF-8
a1.sources.r1.portHeader = port

#k1 hdfs sink configuration
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.fileType = DataStream
#a1.sinks.k1.hdfs.charset.default = UTF-8
a1.sinks.k1.hdfs.path = hdfs://192.168.54.181:8020/user/cloudera/syslogFlumeHdfsSpark/%{IP}/%Y%m%d
a1.sinks.k1.hdfs.filePrefix = %{log_type}-%H%M%S
a1.sinks.k1.hdfs.fileSuffix = .txt
a1.sinks.k1.hdfs.idleTimeout = 0
a1.sinks.k1.hdfs.rollSize = 10

#k2 spark streaming sink configuration
a1.sinks.k2.type = org.apache.spark.streaming.flume.sink.SparkSink

a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000

a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1



